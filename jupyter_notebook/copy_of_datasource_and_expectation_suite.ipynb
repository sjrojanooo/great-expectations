{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22b79e5e",
   "metadata": {},
   "source": [
    "# Create a new spark Datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5e731161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ruamel import yaml\n",
    "import great_expectations as gx\n",
    "from great_expectations.cli.datasource import sanitize_yaml_and_save_datasource, check_if_datasource_name_exists\n",
    "from great_expectations.core.batch import BatchRequest, RuntimeBatchRequest\n",
    "from great_expectations.core.expectation_configuration import ExpectationConfiguration\n",
    "from great_expectations.exceptions import DataContextError\n",
    "from great_expectations.data_context.util import file_relative_path\n",
    "from great_expectations.validator.validator import Validator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0d1f39f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the great expectations context. This is very similar to activating a session \n",
    "context = gx.get_context()\n",
    "gx_data_context = gx.data_context.DataContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05f3666",
   "metadata": {},
   "source": [
    "## Customize Your Datasource Configuration\n",
    "\n",
    "**If you are new to Great Expectations Datasources,** you should check out our [how-to documentation](https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/connect_to_data_overview)\n",
    "\n",
    "**My configuration is not so simple - are there more advanced options?**\n",
    "Glad you asked! Datasources are versatile. Please see our [How To Guides](https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/connect_to_data_overview)!\n",
    "\n",
    "Give your datasource a unique name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "89c0b221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to instantiate class from config...\n",
      "\tInstantiating as a Datasource, since class_name is Datasource\n",
      "\tSuccessfully instantiated Datasource\n",
      "\n",
      "\n",
      "ExecutionEngine class name: SparkDFExecutionEngine\n",
      "Data Connectors:\n",
      "\tdefault_inferred_data_connector_name : InferredAssetFilesystemDataConnector\n",
      "\n",
      "\tAvailable data_asset_names (2 of 2):\n",
      "\t\tadidas_us_retail_sales_data-converted (1 of 1): ['adidas_us_retail_sales_data-converted.csv']\n",
      "\t\tadidas_us_retail_sales_data-raw (1 of 1): ['adidas_us_retail_sales_data-raw.csv']\n",
      "\n",
      "\tUnmatched data_references (0 of 0):[]\n",
      "\n",
      "\tdefault_runtime_data_connector_name:RuntimeDataConnector\n",
      "\n",
      "\tAvailable data_asset_names (0 of 0):\n",
      "\t\tNote : RuntimeDataConnector will not have data_asset_names until they are passed in through RuntimeBatchRequest\n",
      "\n",
      "\tUnmatched data_references (0 of 0): []\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<great_expectations.datasource.new_datasource.Datasource at 0x7fac2954bd60>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasource_yaml = rf\"\"\"\n",
    "name: adidas_retail_sales\n",
    "class_name: Datasource\n",
    "execution_engine:\n",
    "    class_name: SparkDFExecutionEngine\n",
    "data_connectors:\n",
    "    default_runtime_data_connector_name:\n",
    "        class_name: RuntimeDataConnector\n",
    "        batch_identifiers:\n",
    "            - default_identifier_name\n",
    "    default_inferred_data_connector_name:\n",
    "        class_name: InferredAssetFilesystemDataConnector\n",
    "        base_directory:  ../data/\n",
    "        default_regex:\n",
    "            group_names:\n",
    "                - data_asset_name\n",
    "            pattern: (.*)\\.csv\n",
    "\"\"\"\n",
    "\n",
    "context.test_yaml_config(yaml_config=datasource_yaml)\n",
    "context.add_datasource(**yaml.load(datasource_yaml))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb9e246",
   "metadata": {},
   "source": [
    "# Connect to our Data \n",
    "\n",
    "#### RuntimeDataConnector and RuntimeBatchRequest\n",
    "* If we analyzed the yaml the configured yaml file above we can see that under `data_connectors` we have our class_name set as RuntimeDataConnector. This is a special kind of connector used to enable RuntimeBatchRequest, that allow you to use in_memory dataframes, sql queries, and even a file path to use are your data source when validating the expectations. \n",
    "* Below we will the configuration required to process an in_memory dataframe used to pass this request into the context validator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6d0cf0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here you could actually pass all of the file inside of the file path you provide under runtime paramters. \n",
    "# we just want to use the converted version \n",
    "from pyspark.sql import SparkSession \n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "suite_name = \"adidas_test_suite\"\n",
    "\n",
    "in_memory_df = spark.read.csv(\"../../data/adidas_us_retail_sales_data-converted.csv\", header=True, sep=',')\n",
    "\n",
    "runtime_batch_request = RuntimeBatchRequest(\n",
    "    datasource_name=\"adidas_retail_sales\",\n",
    "    data_connector_name=\"default_runtime_data_connector_name\",\n",
    "    data_asset_name=\"adidas_retail_sales\",  # This can be anything that identifies this data_asset for you\n",
    "    runtime_parameters={\"batch_data\": df},  # Add your path here.\n",
    "    batch_identifiers={\"default_identifier_name\": \"adidas.retail_sales\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b028341",
   "metadata": {},
   "source": [
    "# Create Expectation Suite \n",
    "### What are we doing? \n",
    "I am using spark to connect to our filesystem and wanted to provide a set of paramater to my batch request. The RuntimeBatchRequest is not as limited as a BatchRequest. We provide similar paramters as BatchRequest but can also apply the reader method, and options in our request. I found this highly useful, since there are time where we are expecting files with column names, and not just default index value names when validating data sources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1f4284c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9648\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd2b7a49dc24ea49ce8d48c0313cdae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['invoice_date', 'retailer', 'retailer_id', 'region', 'state', 'city', 'product', 'price_per_unit', 'units_sold', 'total_sales', 'operating_profit', 'operating_margin', 'sales_method']\n",
      "+------------+-----------+-----------+---------+--------+--------+-------------------------+--------------+----------+-----------+----------------+----------------+------------+\n",
      "|invoice_date|retailer   |retailer_id|region   |state   |city    |product                  |price_per_unit|units_sold|total_sales|operating_profit|operating_margin|sales_method|\n",
      "+------------+-----------+-----------+---------+--------+--------+-------------------------+--------------+----------+-----------+----------------+----------------+------------+\n",
      "|2020-01-01  |Foot Locker|1185732    |Northeast|New York|New York|Men's Street Footwear    |50.0          |1200.0    |600000.0   |300000.0        |50.0            |In-store    |\n",
      "|2020-01-02  |Foot Locker|1185732    |Northeast|New York|New York|Men's Athletic Footwear  |50.0          |1000.0    |500000.0   |150000.0        |30.0            |In-store    |\n",
      "|2020-01-03  |Foot Locker|1185732    |Northeast|New York|New York|Women's Street Footwear  |40.0          |1000.0    |400000.0   |140000.0        |35.0            |In-store    |\n",
      "|2020-01-04  |Foot Locker|1185732    |Northeast|New York|New York|Women's Athletic Footwear|45.0          |850.0     |382500.0   |133875.0        |35.0            |In-store    |\n",
      "|2020-01-05  |Foot Locker|1185732    |Northeast|New York|New York|Men's Apparel            |60.0          |900.0     |540000.0   |162000.0        |30.0            |In-store    |\n",
      "+------------+-----------+-----------+---------+--------+--------+-------------------------+--------------+----------+-----------+----------------+----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# validator replaces the batch in v3 version \n",
    "# first we create our suite\n",
    "\n",
    "context.create_expectation_suite(\n",
    "    expectation_suite_name = suite_name, \n",
    "    overwrite_existing=True\n",
    ")\n",
    "\n",
    "validator = context.get_validator(\n",
    "    batch_request=runtime_batch_request, \n",
    "    expectation_suite_name=suite_name\n",
    ")\n",
    "\n",
    "validator_df = validator.active_batch.data.dataframe\n",
    "print(validator_df.count())\n",
    "column_names = [column for column in validator.columns()]\n",
    "print(column_names)\n",
    "\n",
    "validator_df.limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8a53b41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Expectation\n",
    "expectation_configs = ExpectationConfiguration(\n",
    "   # Name of expectation type being added\n",
    "   expectation_type=\"expect_table_columns_to_match_ordered_list\",\n",
    "   # These are the arguments of the expectation\n",
    "   # The keys allowed in the dictionary are Parameters and\n",
    "   # Keyword Arguments of this Expectation Type\n",
    "   kwargs={\n",
    "      \"column_list\": ['invoice_date', 'retailer', 'retailer_id', \n",
    "                      'region', 'state', 'city', 'product', 'price_per_unit',\n",
    "                      'units_sold', 'total_sales', 'operating_profit', \n",
    "                      'operating_margin', 'sales_method']\n",
    "   },\n",
    "   # This is how you can optionally add a comment about this expectation.\n",
    "   # It will be rendered in Data Docs.\n",
    "   # See this guide for details:\n",
    "   # `How to add comments to Expectations and display them in Data Docs`.\n",
    "   meta={\n",
    "      \"notes\": {\n",
    "         \"format\": \"markdown\",\n",
    "         \"content\": \"columns must appear in this order\"\n",
    "      }\n",
    "   }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "512c4da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"kwargs\": {\"column_list\": [\"invoice_date\", \"retailer\", \"retailer_id\", \"region\", \"state\", \"city\", \"product\", \"price_per_unit\", \"units_sold\", \"total_sales\", \"operating_profit\", \"operating_margin\", \"sales_method\"]}, \"expectation_type\": \"expect_table_columns_to_match_ordered_list\", \"meta\": {\"notes\": {\"format\": \"markdown\", \"content\": \"columns must appear in this order\"}}}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validator._expectation_suite.add_expectation(expectation_configuration=expectation_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44abd688",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
